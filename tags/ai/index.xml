<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>AI on My Blog</title>
    <link>https://seconsorl.github.io/tags/ai/</link>
    <description>Recent content in AI on My Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <managingEditor>216002917@nbu.edu.cn (HZW)</managingEditor>
    <webMaster>216002917@nbu.edu.cn (HZW)</webMaster>
    <lastBuildDate>Mon, 06 May 2024 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://seconsorl.github.io/tags/ai/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>DeepLearning</title>
      <link>https://seconsorl.github.io/post/deeplearning/</link>
      <pubDate>Mon, 06 May 2024 00:00:00 +0000</pubDate><author>216002917@nbu.edu.cn (HZW)</author>
      <guid>https://seconsorl.github.io/post/deeplearning/</guid>
      <description>TIPS 机器学习模型训练的步骤：&#xA;1、数据模块（数据采集，清洗，处理等） 2、建立模型（各种模型的建立） 3、损失函数的选择（根据不同的任务选择不同的损失函数），有了loss就可以求取梯度 4、得到梯度之后，我们会选择某种优化方式去进行优化 5、然后迭代训练 线性回归 介绍 线性回归是分析一个变量与另外一(多)个变量之间关系的方法。因变量是$y$，自变量是$x$，关系线性 $$ y=w\times x +b $$ 任务就是求解$w$，$b$。&#xA;求解步骤： 确定模型&#xA;选择损失函数：这里用MSE $$ \frac{1}{m} \sum_{i=1}^{m}(y_i-\hat{y_i}) $$&#xA;求解梯度并更新$w$，$b$ $$ w = w - LR \times w.grad \ b = b - LR \times w.grad $$&#xA;代码 # 首先我们得有训练样本X，Y， 这里我们随机生成 x = torch.rand(20, 1) * 10 y = 2 * x + (5 + torch.randn(20, 1)) # 构建线性回归函数的参数 w = torch.randn((1), requires_grad=True) b = torch.zeros((1), requires_grad=True) # 这俩都需要求梯度 for iteration in range(100): # 前向传播 wx = torch.mul(w, x) y_pred = torch.add(wx, b) # 计算loss loss = (0.5 * (y-y_pred)**2).mean() # 反向传播 loss.backward() # 更新参数 b.data.sub_(lr * b.</description>
    </item>
    <item>
      <title>Fine tuning</title>
      <link>https://seconsorl.github.io/post/fine-tuning/</link>
      <pubDate>Mon, 06 May 2024 00:00:00 +0000</pubDate><author>216002917@nbu.edu.cn (HZW)</author>
      <guid>https://seconsorl.github.io/post/fine-tuning/</guid>
      <description>TIPS https://zhuanlan.zhihu.com/p/673789772&#xA;介绍 微调是指调整大型语言模型（LLM）的参数以适应特定任务的过程。这是通过在与任务相关的数据集上训练模型来完成的。所需的微调量取决于任务的复杂性和数据集的大小。&#xA;在深度学习中，微调是一种重要的技术，用于改进预训练模型的性能。除了微调ChatGPT之外，还有许多其他预训练模型可以进行微调。&#xA;PEFT PEFT（Parameter-Efficient Fine-Tuning）是hugging face开源的一个参数高效微调大模型的工具，里面集成了4种微调大模型的方法，可以通过微调少量参数就达到接近微调全量参数的效果，使得在GPU资源不足的情况下也可以微调大模型。&#xA;微调方法 微调可以分为全微调和重用两个方法：&#xA;全微调（Full Fine-tuning）：全微调是指对整个预训练模型进行微调，包括所有的模型参数。在这种方法中，预训练模型的所有层和参数都会被更新和优化，以适应目标任务的需求。这种微调方法通常适用于任务和预训练模型之间存在较大差异的情况，或者任务需要模型具有高度灵活性和自适应能力的情况。Full Fine-tuning需要较大的计算资源和时间，但可以获得更好的性能。 部分微调（Repurposing）：部分微调是指在微调过程中只更新模型的顶层或少数几层，而保持预训练模型的底层参数不变。这种方法的目的是在保留预训练模型的通用知识的同时，通过微调顶层来适应特定任务。Repurposing通常适用于目标任务与预训练模型之间有一定相似性的情况，或者任务数据集较小的情况。由于只更新少数层，Repurposing相对于Full Fine-tuning需要较少的计算资源和时间，但在某些情况下性能可能会有所降低。 微调预训练模型的方法： 微调所有层：将预训练模型的所有层都参与微调，以适应新的任务。 微调顶层：只微调预训练模型的顶层，以适应新的任务。 冻结底层：将预训练模型的底层固定不变，只对顶层进行微调。 逐层微调：从底层开始，逐层微调预训练模型，直到所有层都被微调。 迁移学习：将预训练模型的知识迁移到新的任务中，以提高模型性能。这种方法通常使用微调顶层或冻结底层的方法。 Fine tuning 经典的Fine tuning方法包括将预训练模型与少量特定任务数据一起继续训练。在这个过程中，预训练模型的权重被更新，以更好地适应任务。所需的Fine-tuning量取决于预训练语料库和任务特定语料库之间的相似性。如果两者相似，可能只需要少量的Fine tuning。如果两者不相似，则可能需要更多的Fine tuning。&#xA;![img](./Fine tuning_img/v2-763deff2ae9439f3c415a39dfbbc7b1d_1440w.jpg)&#xA;Prompt Tuning（P-tuning） Prompt Tuning 是2021年谷歌在论文《The Power of Scale for Parameter-Efficient Prompt Tuning》中提出的微调方法。参数高效性微调方法中实现最简单的方法还是Prompt tuning(也就是我们常说的P-Tuning)，固定模型前馈层参数，仅仅更新部分embedding参数即可实现低成本微调大模型。&#xA;![img](./Fine tuning_img/v2-4407ecbc3bf703aad37c5de9633dede7_1440w.jpg)&#xA;经典的Prompt tuning方式不涉及对底层模型的任何参数更新。相反，它侧重于精心制作可以指导预训练模型生成所需输出的输入提示或模板。主要结构是利用了一个prompt encoder（BiLSTM+MLP），将一些pseudo prompt先encode（离散token）再与input embedding进行拼接，同时利用LSTM进行 Reparamerization 加速训练，并引入少量自然语言提示的锚字符（Anchor，例如Britain）进一步提升效果。然后结合（capital，Britain）生成得到结果，再优化生成的encoder部分。&#xA;![img](./Fine tuning_img/v2-632a4b208c25a1919c459b1b72ba3dfd_1440w.jpg)&#xA;但是P-tuning v1有两个显著缺点：任务不通用和规模不通用。在一些复杂的自然语言理解NLU任务上效果很差，同时预训练模型的参数量不能过小。具体的效果论文中提到以下几点：&#xA;Prompt 长度影响：模型参数达到一定量级时，Prompt 长度为1也能达到不错的效果，Prompt 长度为20就能达到极好效果。 Prompt初始化方式影响：Random Uniform 方式明显弱于其他两种，但是当模型参数达到一定量级，这种差异也不复存在。 预训练的方式：LM Adaptation 的方式效果好，但是当模型达到一定规模，差异又几乎没有了。 微调步数影响：模型参数较小时，步数越多，效果越好。同样随着模型参数达到一定规模，zero shot 也能取得不错效果。 当参数达到100亿规模与全参数微调方式效果无异。 代码示例： from peft import PromptTuningConfig, get_peft_model peft_config = PromptTuningConfig(task_type=&amp;#34;SEQ_CLS&amp;#34;, num_virtual_tokens=10) model = AutoModelForCausalLM.from_pretrained(model_name_or_path, return_dict=True) model = get_peft_model(model, peft_config) Prefix Tuning 2021年论文《Prefix-Tuning: Optimizing Continuous Prompts for Generation》中提出了 Prefix Tuning 方法。与Full-finetuning 更新所有参数的方式不同，该方法是在输入 token 之前构造一段任务相关的 virtual tokens 作为 Prefix，然后训练的时候只更新 Prefix 部分的参数，而 Transformer 中的其他部分参数固定。</description>
    </item>
    <item>
      <title>pytorch</title>
      <link>https://seconsorl.github.io/post/pytorch%E5%AD%A6%E4%B9%A0/</link>
      <pubDate>Mon, 06 May 2024 00:00:00 +0000</pubDate><author>216002917@nbu.edu.cn (HZW)</author>
      <guid>https://seconsorl.github.io/post/pytorch%E5%AD%A6%E4%B9%A0/</guid>
      <description>TIPS 中文文档&#xA;学习顺序&#xA;数据模块 -&amp;gt; 模型模块 -&amp;gt; 损失函数 -&amp;gt; 优化器 -&amp;gt; 迭代训练&#xA;下载和安装&#xA;直接pip install pytorch torch.Storage 一个torch.Storage是一个单一数据类型的连续一维数组。&#xA;每个torch.Tensor都有一个对应的、相同数据类型的存储。&#xA;具体数据类型见下表&#xA;类型 描述 类型 描述 byte()、char()、double()、float()、half()、int()、long()、short() 将此存储转为相应类型 cpu() 如果当前此存储不在CPU上，则返回一个它的CPU副本 pin_memory() 如果此存储当前未被锁定，则将它复制到锁定内存中。 cuda(device=None, async=False) 返回此对象在CUDA内存中的一个副本。 如果此对象已在CUDA内存中且在正确的设备上，那么不会执行复制操作，直接返回原对象。参数：&#xA;device(int) - 目标GPU的id。默认值是当前设备。async (bool) -如果值为True，且源在锁定内存中，则副本相对于宿主是异步的。否则此参数不起效果。 clone()、copy_() 返回此存储的一个副本 tolist() 返回一个包含此存储中元素的列表 type(new_type=None, async=False) 将此对象转为指定类型。如果已经是正确类型，不会执行复制操作，直接返回原对象。 size() 大小 张量（tensor） Torch 自称为神经网络界的Numpy, 因为他能将 torch 产生的 tensor 放在 GPU 中加速运算 (前提是你有合适的 GPU), 就像 Numpy 会把 array 放在 CPU 中加速运算。（numpy array 和 torch tensor 是可以自由地转换的。）&#xA;定义 pytorch中的数据结构——Tensor，Tensor是PyTorch中最基础的概念。&#xA;张量其实是一个多维数组，它是标量、向量、矩阵的高维拓展&#xA;Tensor与Variable&#xA;在Pytorch0.4.0版本之后其实Variable已经并入Tensor，Variable是torch.autograd中的数据类型&#xA;Variable的属性 data: 被包装的Tensor&#xA;grad: data的梯度&#xA;grad_fn: fn表示function的意思，记录创建的创建张量时用到的方法，比如说加法，乘法，这个操作在求导过程需要用到&#xA;Tensor的Function， 是自动求导的关键&#xA;requires_grad: 指示是否需要梯度， 有的不需要梯度&#xA;is_leaf: 指示是否是叶子节点（张量）&#xA;Tensor的属性 Tensor共有8个属性，其中有5个是Variable并入过来的，剩下三个如下：&#xA;dtype: 张量的数据类型， 如torch.FloatTensor, torch.cuda.FloatTensor, 用的最多的一般是float32和int64(torch.long)&#xA;shape: 张量的形状， 如(64, 3, 224, 224)</description>
    </item>
  </channel>
</rss>
