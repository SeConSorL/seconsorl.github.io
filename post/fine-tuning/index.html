<!DOCTYPE html>
<html lang="en" data-theme="light"><head>
    <title>Fine tuning · HZW</title>
    <meta charset="utf-8">
    
    <meta name="generator" content="Hugo 0.124.1">
    <meta property="og:title" content="Fine tuning" />
<meta property="og:description" content="a note for learning Fine tuning" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://seconsorl.github.io/post/fine-tuning/" /><meta property="og:image" content="https://seconsorl.github.io/images/profile.png" /><meta property="article:section" content="post" />
<meta property="article:published_time" content="2024-05-06T00:00:00+00:00" />
<meta property="article:modified_time" content="2024-05-06T00:00:00+00:00" /><meta property="og:site_name" content="A junior from NBU" />



    <meta name="viewport" content="width=device-width,initial-scale=1,viewport-fit=cover">
    <meta name="description" content="HZW">
    
    
    
    <link rel="stylesheet" type="text/css" href="https://seconsorl.github.io/css/style.min.565d8c479597aa43658922d4b31e286529a7525a22c9546fa1018fc5e5ef6d86.css" integrity="sha256-Vl2MR5WXqkNliSLUsx4oZSmnUloiyVRvoQGPxeXvbYY=" crossorigin="anonymous" type="text/css">

    
    
    
    <script type="text/javascript" src="https://seconsorl.github.io/js/heyo-header.min.a3fa728a9f57833a31dfb45c48caaf1e4890c8c97f07bd7133fc2359745edb5d.js" integrity="sha256-o/pyip9Xgzox37RcSMqvHkiQyMl/B71xM/wjWXRe210=" crossorigin="anonymous"></script>

    
    
    <link rel="stylesheet" type="text/css" href="https://seconsorl.github.io/css/fonts.9398921f2d404983c2b7f9a68ddc72e3f5e58a3e38b0a8e4a70d75c12ebfb7c5.css" integrity="sha256-k5iSHy1ASYPCt/mmjdxy4/Xlij44sKjkpw11wS6/t8U=" crossorigin="anonymous">

    
    
    
    <script type="text/javascript" src="https://seconsorl.github.io/js/sidebar-toc.min.788b639e2ec681549740b90b3b865d5f9e1789e3ca9c06ccc45d65655434c954.js" integrity="sha256-eItjni7GgVSXQLkLO4ZdX54XiePKnAbMxF1lZVQ0yVQ=" crossorigin="anonymous"></script>

    
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/p5.js/1.1.9/p5.min.js" defer></script>

        
        
        <script type="text/javascript" src="https://seconsorl.github.io/js/sketch-graph.26b92ed9317bdc6f35642d588bdf3283f40998846e01cf4bee22a126907fbf3b.js" integrity="sha256-Jrku2TF73G81ZC1Yi98yg/QJmIRuAc9L7iKhJpB/vzs=" crossorigin="anonymous" defer></script>

        
        
        <script type="text/javascript" src="https://seconsorl.github.io/js/sketch-digitalRain.af8a7b5c4428cc62d5bf49bf2698d4112c2459ee0c22c1c753ab304aef69888a.js" integrity="sha256-r4p7XEQozGLVv0m/JpjUESwkWe4MIsHHU6swSu9piIo=" crossorigin="anonymous" defer></script>

        
        
        <script type="text/javascript" src="https://seconsorl.github.io/js/sketch-circleBrushStrokes.fe8fc3ee52e1d90e9236be8c36a27711efa024beb4da304829f95dfbb61d6e84.js" integrity="sha256-/o/D7lLh2Q6SNr6MNqJ3Ee&#43;gJL602jBIKfld&#43;7YdboQ=" crossorigin="anonymous" defer></script>

        
        
        <script type="text/javascript" src="https://seconsorl.github.io/js/sketch-meta.71b5202ea881c86ac19e4b55414656a5444204a4ba08ff7368a5aa99c0a60949.js" integrity="sha256-cbUgLqiByGrBnktVQUZWpURCBKS6CP9zaKWqmcCmCUk=" crossorigin="anonymous" defer></script>

        
        
        <script type="text/javascript" src="https://seconsorl.github.io/js/sidebar-sketch.min.2e95015880993ef9abcad62d111decea22406616931bce193254bf8af2339953.js" integrity="sha256-LpUBWICZPvmrytYtER3s6iJAZhaTG84ZMlS/ivIzmVM=" crossorigin="anonymous" defer></script>
    
    
    
    <link rel="shortcut icon" href="https://seconsorl.github.io/favicons/favicon.ico" type="image/x-icon">
    <link rel="apple-touch-icon" sizes="180x180" href="https://seconsorl.github.io/favicons/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="https://seconsorl.github.io/favicons/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="https://seconsorl.github.io/favicons/favicon-16x16.png">
    <link rel="canonical" href="https://seconsorl.github.io/post/fine-tuning/">
    
    
    
    
    

    
    <meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:image" content="https://seconsorl.github.io/images/profile.png" /><meta name="twitter:title" content="Fine tuning"/>
<meta name="twitter:description" content="a note for learning Fine tuning"/>

</head><body>
        <div class="main">
            <div class="page-top">
    <a role="button" class="navbar-burger" data-target="navMenu" aria-label="menu" aria-expanded="false" >
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
      </a>
    <ul class="nav" id="navMenu">
        
        
            
            <li><a  href="/"  title="">Home</a></li>
        
            
            <li><a  href="/post/"  title="">Posts</a></li>
        
            
            <li><a  href="/about/"  title="">About</a></li>
        
        <li class="grow"></li>
        
        <li>
            <a class="theme-switch" title="Switch Theme">
                <i class="fa fa-adjust fa-fw" aria-hidden="true"></i>
            </a>
        </li>
    </ul>
</div>
            <div class="sidebar" id="sidebar">
    <div class="top-toc">
        <img src="https://seconsorl.github.io/images/profile.png" alt="profile picture">
        
        <a href="/">A junior from NBU</a>
    </div>
    
    <div class="middle-sidebar grow" id="middle-sidebar">
        
            
            
                
            

            
        
    </div>

    <div class="footer">
        <ul class="social-links">
            
            <li>
                <a href="https://linkedin.com/" target="_blank" rel="noopener noreferrer" rel="me" aria-label="Linkedin">
                    <i class="fab fa-linkedin" aria-hidden="true"></i>
                </a>
            </li>
            
            <li>
                <a href="https://github.com/SeConSorL" target="_blank" rel="noopener noreferrer" rel="me" aria-label="GitHub">
                    <i class="fab fa-github" aria-hidden="true"></i>
                </a>
            </li>
            
            <li>
                <a href="https://www.instagram.com/" target="_blank" rel="noopener noreferrer" rel="me" aria-label="instagram">
                    <i class="fab fa-instagram" aria-hidden="true"></i>
                </a>
            </li>
            
            <li>
                <a href="mailto:216002917@nbu.edu.cn" target="_blank" rel="noopener noreferrer" rel="me" aria-label="e-mail">
                    <i class="fas fa-envelope" aria-hidden="true"></i>
                </a>
            </li>
            
        </ul>

        <div class="by">by HZW <b>·</b> 2025</div>
    </div>
</div>
            <div class="content">
<div class="post">
    
    <div class="thumbnail" style="box-shadow: var(--box-shadow); height: 350px;">
        <img src=https://seconsorl.github.io/images/Fine_tuning.jpg style="object-position: 50% 100%;" title=Fine_tuning alt=Fine_tuning loading="lazy">
    </div>
    
    <div class="post-title">
        <h1>Fine tuning</h1>
        
            <div class="post-header">
    <div style="padding-top: 10px;">
        <i class="far fa-calendar"></i><span class="date">May 6, 2024</span>
        <i class="far fa-clock"></i><span class="reading-time">2 minutes</span>
        


    </div>
</div>
        
    </div>
    <div class="post-content">
        <h2 id="tips">TIPS</h2>
<p><a href="https://zhuanlan.zhihu.com/p/673789772">https://zhuanlan.zhihu.com/p/673789772</a></p>
<h2 id="介绍">介绍</h2>
<p><a href="https://zhida.zhihu.com/search?content_id=237860613&content_type=Article&match_order=1&q=微调&zhida_source=entity">微调</a>是指调整大型语言模型（LLM）的参数以适应特定任务的过程。这是通过在与任务相关的数据集上训练模型来完成的。所需的微调量取决于任务的复杂性和数据集的大小。</p>
<p>在深度学习中，微调是一种重要的技术，用于改进预训练模型的性能。除了微调ChatGPT之外，还有许多其他预训练模型可以进行微调。</p>
<h2 id="peft">PEFT</h2>
<p>PEFT（Parameter-Efficient Fine-Tuning）是hugging face开源的一个参数高效微调大模型的工具，里面集成了4种微调大模型的方法，可以通过微调少量参数就达到接近微调全量参数的效果，使得在GPU资源不足的情况下也可以微调大模型。</p>
<h2 id="微调方法">微调方法</h2>
<p>微调可以分为全微调和重用两个方法：</p>
<ol>
<li>全微调（Full Fine-tuning）：全微调是指对整个预训练模型进行微调，包括所有的模型参数。在这种方法中，预训练模型的所有层和参数都会被更新和优化，以适应目标任务的需求。这种微调方法通常适用于任务和预训练模型之间存在较大差异的情况，或者任务需要模型具有高度灵活性和自适应能力的情况。Full Fine-tuning需要较大的计算资源和时间，但可以获得更好的性能。</li>
<li>部分微调（Repurposing）：部分微调是指在微调过程中只更新模型的顶层或少数几层，而保持预训练模型的底层参数不变。这种方法的目的是在保留预训练模型的通用知识的同时，通过微调顶层来适应特定任务。Repurposing通常适用于目标任务与预训练模型之间有一定相似性的情况，或者任务数据集较小的情况。由于只更新少数层，Repurposing相对于Full Fine-tuning需要较少的计算资源和时间，但在某些情况下性能可能会有所降低。</li>
</ol>
<h2 id="微调预训练模型的方法">微调预训练模型的方法：</h2>
<ul>
<li>微调所有层：将预训练模型的所有层都参与微调，以适应新的任务。</li>
<li>微调顶层：只微调预训练模型的顶层，以适应新的任务。</li>
<li>冻结底层：将预训练模型的底层固定不变，只对顶层进行微调。</li>
<li>逐层微调：从底层开始，逐层微调预训练模型，直到所有层都被微调。</li>
<li>迁移学习：将预训练模型的知识迁移到新的任务中，以提高模型性能。这种方法通常使用微调顶层或冻结底层的方法。</li>
</ul>
<h2 id="fine-tuning">Fine tuning</h2>
<p>经典的Fine tuning方法包括将预训练模型与少量特定任务数据一起继续训练。在这个过程中，预训练模型的权重被更新，以更好地适应任务。所需的Fine-tuning量取决于预训练语料库和任务特定语料库之间的相似性。如果两者相似，可能只需要少量的Fine tuning。如果两者不相似，则可能需要更多的Fine tuning。</p>
<p>![img](./Fine tuning_img/v2-763deff2ae9439f3c415a39dfbbc7b1d_1440w.jpg)</p>
<h2 id="prompt-tuningp-tuning">Prompt Tuning（P-tuning）</h2>
<p>Prompt Tuning 是2021年谷歌在论文《The Power of Scale for Parameter-Efficient Prompt Tuning》中提出的微调方法。参数高效性微调方法中实现最简单的方法还是Prompt tuning(也就是我们常说的P-Tuning)，固定模型前馈层参数，仅仅更新部分embedding参数即可实现低成本微调大模型。</p>
<p>![img](./Fine tuning_img/v2-4407ecbc3bf703aad37c5de9633dede7_1440w.jpg)</p>
<p>经典的Prompt tuning方式不涉及对底层模型的任何参数更新。相反，它侧重于精心制作可以指导预训练模型生成所需输出的输入提示或模板。主要结构是利用了一个prompt encoder（BiLSTM+MLP），将一些pseudo prompt先encode（离散token）再与input embedding进行拼接，同时利用LSTM进行 Reparamerization 加速训练，并引入少量自然语言提示的锚字符（Anchor，例如Britain）进一步提升效果。然后结合（capital，Britain）生成得到结果，再优化生成的encoder部分。</p>
<p>![img](./Fine tuning_img/v2-632a4b208c25a1919c459b1b72ba3dfd_1440w.jpg)</p>
<p>但是P-tuning v1有两个显著缺点：任务不通用和规模不通用。在一些复杂的自然语言理解NLU任务上效果很差，同时预训练模型的参数量不能过小。具体的效果论文中提到以下几点：</p>
<ul>
<li>Prompt 长度影响：模型参数达到一定量级时，Prompt 长度为1也能达到不错的效果，Prompt 长度为20就能达到极好效果。</li>
<li>Prompt初始化方式影响：Random Uniform 方式明显弱于其他两种，但是当模型参数达到一定量级，这种差异也不复存在。</li>
<li>预训练的方式：LM Adaptation 的方式效果好，但是当模型达到一定规模，差异又几乎没有了。</li>
<li>微调步数影响：模型参数较小时，步数越多，效果越好。同样随着模型参数达到一定规模，zero shot 也能取得不错效果。</li>
<li>当参数达到100亿规模与全参数微调方式效果无异。</li>
</ul>
<h3 id="代码示例">代码示例：</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python3" data-lang="python3"><span style="display:flex;"><span><span style="color:#ff79c6">from</span> peft <span style="color:#ff79c6">import</span> PromptTuningConfig, get_peft_model
</span></span><span style="display:flex;"><span>peft_config <span style="color:#ff79c6">=</span> PromptTuningConfig(task_type<span style="color:#ff79c6">=</span><span style="color:#f1fa8c">&#34;SEQ_CLS&#34;</span>, num_virtual_tokens<span style="color:#ff79c6">=</span><span style="color:#bd93f9">10</span>)
</span></span><span style="display:flex;"><span>model <span style="color:#ff79c6">=</span> AutoModelForCausalLM<span style="color:#ff79c6">.</span>from_pretrained(model_name_or_path, return_dict<span style="color:#ff79c6">=</span><span style="color:#ff79c6">True</span>)
</span></span><span style="display:flex;"><span>model <span style="color:#ff79c6">=</span> get_peft_model(model, peft_config)
</span></span></code></pre></div><h2 id="prefix-tuninghttpszhidazhihucomsearchcontent_id237860613content_typearticlematch_order1qprefixtuningzhida_sourceentity"><a href="https://zhida.zhihu.com/search?content_id=237860613&content_type=Article&match_order=1&q=Prefix+Tuning&zhida_source=entity">Prefix Tuning</a></h2>
<p>2021年论文《Prefix-Tuning: Optimizing Continuous Prompts for Generation》中提出了 Prefix Tuning 方法。与Full-finetuning 更新所有参数的方式不同，该方法是在输入 token 之前构造一段任务相关的 virtual tokens 作为 Prefix，然后训练的时候只更新 Prefix 部分的参数，而 Transformer 中的其他部分参数固定。</p>
<p>prefix-tuning技术，相对于fine-tuning，在调节模型的过程中只优化一小段可学习的continuous task-specific vector（prefix）而不是整个模型的参数。该方法其实和构造 Prompt 类似，只是 Prompt 是人为构造的“显式”的提示，并且无法更新参数，而Prefix 则是可以学习的“隐式”的提示。手动尝试最优的提示无异于大海捞针，于是便有了自动离散提示搜索的方法，但提示是离散的，神经网络是连续的，所以寻找的最优提示可能是次优的。</p>
<p>![img](./Fine tuning_img/v2-8003cafa1676313c3629a82fd9a17082_1440w.jpg)</p>
<h3 id="代码示例-1">代码示例：</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-text" data-lang="text"><span style="display:flex;"><span>peft_config = PrefixTuningConfig(task_type=&#34;CAUSAL_LM&#34;, num_virtual_tokens=20)
</span></span><span style="display:flex;"><span>model = AutoModelForCausalLM.from_pretrained(model_name_or_path, return_dict=True)
</span></span><span style="display:flex;"><span>model = get_peft_model(model, peft_config)
</span></span></code></pre></div><p>GPT在P-tuning的加持下可达到甚至超过BERT在NLU领域的性能。下图是细致的对比：</p>
<p>![img](./Fine tuning_img/v2-6d3891246e2c95398d1c3fed23766e15_1440w.jpg)</p>
<h2 id="p-tuning-v2">P-tuning v2</h2>
<p>V2版本主要是基于P-tuning和prefix-tuning技术，引入Deep Prompt Encoding和Multi-task Learning等策略进行优化的。实验表明，仅精调0.1%参数量，在330M到10B不同参数规模LM模型上，均取得和Fine-tuning相比肩的性能。</p>
<p>![img](./Fine tuning_img/v2-45e5341a2f278d80d828d782a983a29b_1440w.jpg)</p>
<p>论文《P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks》从标题就可以看出，P-Tuning v2 的目标就是要让 Prompt Tuning 能够在不同参数规模的预训练模型、针对不同下游任务的结果上都达到匹敌 Fine-tuning 的结果。也就是说当前 Prompt Tuning 方法在这两个方面都存在局限性。</p>
<blockquote>
<p>不同模型规模：Prompt Tuning 和 P-tuning 这两种方法都是在预训练模型参数规模够足够大时，才能达到和Fine-tuning 类似的效果，而参数规模较小时效果则很差。
不同任务类型：Prompt Tuning 和 P-tuning 这两种方法在 sequence tagging 任务上表现都很差。</p>
</blockquote>
<p>![img](./Fine tuning_img/v2-d5a8e6da6da9587ece96fa0d99f6034a_1440w.jpg)</p>
<p>v1到v2的可视化：蓝色部分为参数冻结，橙色部分为可训练部分，可以看到右侧的p-tuning v2中，将continuous prompt加在序列前端，并且每一层都加入可训练的prompts。在左图v1模型中，只将prompt插入input embedding中，会导致可训练的参数被句子的长度所限制。此外P-Tuning v2还包括以下改进：</p>
<ul>
<li>移除了Reparamerization加速训练方式；</li>
<li>采用了多任务学习优化：基于多任务数据集的Prompt进行预训练，然后再适配的下游任务。</li>
<li>舍弃了词汇Mapping的Verbalizer的使用，重新利用[CLS]和字符标签，跟传统finetune一样利用cls或者token的输出做NLU，以增强通用性，可以适配到序列标注任务。</li>
</ul>
<p>P-Tuning v2几个关键设计因素：</p>
<ul>
<li>Reparameterization：Prefix Tuning 和 P-tuning 中都有 MLP 来构造可训练的 embedding。论文发现在自然语言理解领域，面对不同的任务以及不同的数据集，这种方法可能带来完全相反的结论。</li>
<li>Prompt Length： 不同的任务对应的最合适的 Prompt Length 不一样，比如简单分类任务下 length=20 最好，而复杂的任务需要更长的 Prompt Length。</li>
<li>Multi-task Learning 多任务对于 P-Tuning v2 是可选的，但可以利用它提供更好的初始化来进一步提高性能。</li>
<li>Classification Head 使用 LM head 来预测动词是 Prompt Tuning 的核心，但我们发现在完整的数据设置中没有必要这样做，并且这样做与序列标记不兼容。P-tuning v2 采用和 BERT 一样的方式，在第一个 token 处应用随机初始化的分类头。</li>
</ul>
<h3 id="代码示例-2">代码示例：</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-text" data-lang="text"><span style="display:flex;"><span>peft_config = PrefixTuningConfig(task_type=&#34;SEQ_CLS&#34;, num_virtual_tokens=20)
</span></span><span style="display:flex;"><span>model = AutoModelForSequenceClassification.from_pretrained(model_name_or_path, return_dict=True)
</span></span><span style="display:flex;"><span>model = get_peft_model(model, peft_config)
</span></span></code></pre></div><h2 id="adalorahttpszhidazhihucomsearchcontent_id237860613content_typearticlematch_order1qadalorazhida_sourceentity"><a href="https://zhida.zhihu.com/search?content_id=237860613&content_type=Article&match_order=1&q=AdaLoRA&zhida_source=entity">AdaLoRA</a></h2>
<p>预训练语言模型中的不同权重参数对下游任务的贡献是不同的。因此需要更加智能地分配参数预算，以便在微调过程中更加高效地更新那些对模型性能贡献较大的参数。</p>
<p>具体来说，通过奇异值分解将权重矩阵分解为增量矩阵，并根据新的重要性度量动态地调整每个增量矩阵中奇异值的大小。这样可以使得在微调过程中只更新那些对模型性能贡献较大或必要的参数，从而提高了模型性能和参数效率。</p>
<h3 id="代码示例-3">代码示例：</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-text" data-lang="text"><span style="display:flex;"><span>peft_config = AdaLoraConfig(peft_type=&#34;ADALORA&#34;, task_type=&#34;SEQ_2_SEQ_LM&#34;, r=8, lora_alpha=32, target_modules=[&#34;q&#34;, &#34;v&#34;],lora_dropout=0.01)
</span></span><span style="display:flex;"><span>model = AutoModelForCausalLM.from_pretrained(model_name_or_path, return_dict=True)
</span></span><span style="display:flex;"><span>model = get_peft_model(model, peft_config)
</span></span></code></pre></div><h2 id="gpt4模型微调分类">GPT4模型微调分类</h2>
<h3 id="1-adapter-based-methods基于适配器的方法">1. Adapter-based Methods（基于适配器的方法）:</h3>
<p>《Parameter-Efficient Transfer Learning for NLP》提出针对 BERT 的 PEFT微调方式，拉开了 PEFT 研究的序幕。他们指出，在面对特定的下游任务时，如果进行 Full-Fintuning（即预训练模型中的所有参数都进行微调），太过低效；而如果采用固定预训练模型的某些层，只微调接近下游任务的那几层参数，又难以达到较好的效果。</p>
<p>于是他们设计了如下图所示的 Adapter 结构，将其嵌入 Transformer 的结构里面，在训练时，固定住原来预训练模型的参数不变，只对新增的 Adapter 结构进行微调。同时为了保证训练的高效性（也就是尽可能少的引入更多参数），他们将 Adapter 设计为这样的结构：</p>
<blockquote>
<p>首先是一个 down-project 层将高维度特征映射到低维特征；然后过一个非线形层之后，再用一个 up-project 结构将低维特征映射回原来的高维特征；同时也设计了 skip-connection 结构，确保了在最差的情况下能够退化为identity（类似残差结构）。</p>
</blockquote>
<p>这种方法节省了资源，因为它不需要对整个模型进行微调。示例有AdapterDrop、Parallel Adapter、Residual Adapter等。</p>
<p>![img](./Fine tuning_img/v2-1970ac55524151e7f5869fdf20c5ac95_1440w.jpg)</p>
<h3 id="2-prompt-based-methods基于提示的方法">2. Prompt-based Methods（基于提示的方法）:</h3>
<p>这个分支侧重于使用连续的提示（如嵌入向量）来调整模型的行为，而不是直接修改模型的权重。这类方法通常用于生成任务，例如文本生成。提示可以视为模型输入的一部分，它们会被训练以激发模型生成特定的输出。示例包括Prefix-tuning、Prompt tuning等，参加上文介绍。</p>
<h3 id="3-low-rank-adaptation低秩适配">3. Low-rank Adaptation（低秩适配）:</h3>
<p>低秩适配方法致力于将模型权重的改变限制在一个低秩子空间内。这通常涉及对模型的权重矩阵进行分解，只微调其中的一小部分参数。这样可以有效减少计算资源的消耗，同时仍然允许模型有足够的灵活性来学习新任务。LoRA和它的变种，如Q-LoRA、Delta-LoRA、LoRA-FA等，都属于这个类别。</p>
<h3 id="4-sparse-methods稀疏方法">4. Sparse Methods（稀疏方法）:</h3>
<p>Sparse Methods是指仅更新模型中一小部分参数的方法。这些参数被选为最有可能影响到任务性能的，而其他参数则保持不变。稀疏方法的优点在于它们通常能够更高效地利用资源。例如有Intrinsic SAID、Fish Mask、BitFit等。</p>
<p>具体来说，稀疏方法通过选择性地更新权重，减少了计算需求和存储空间，从而提高了效率。这些方法通常用于微调大模型时，以减少参数更新的数量，同时保持模型性能。常见的稀疏方法包括但不限于：</p>
<ul>
<li><strong>知识蒸馏（Knowledge Distillation）</strong>：通过训练一个小模型来模仿一个大模型的输出，从而实现参数的压缩和加速。</li>
<li><strong>模型剪枝（Model Pruning）</strong>：通过删除不重要的权重来减少模型的复杂度，通常通过设置一个阈值来决定哪些权重可以被剪枝。</li>
<li><strong>低秩近似（Low-Rank Approximation）</strong>：通过将权重矩阵分解为低秩矩阵的乘积，从而减少参数的数量。</li>
<li><strong>参数量化（Parameter Quantization）</strong>：通过将浮点数参数转换为更低精度的表示，如INT8，来减少存储和计算需求。</li>
<li><strong>专家混合（Mixture of Experts, MoE）</strong>：通过动态选择一组专家模型来处理输入，而不是使用整个模型，从而实现参数的动态分配和节省。</li>
</ul>
<h3 id="5-others其他方法">5. Others（其他方法）:</h3>
<p>这一分支可能包括不易归类到上述任何一类的其他方法，或者是结合了多种技术的混合方法。这些方法可能包括特定的结构改变、算法优化等，用以提高微调过程的效率或者效果。</p>
<h2 id="大模型微调步骤总结">大模型微调步骤总结</h2>
<p>大模型微调如上文所述有很多方法，并且对于每种方法都会有不同的微调流程、方式、准备工作和周期。然而大部分的大模型微调，都有以下几个主要步骤，并需要做相关的准备：</p>
<h3 id="准备数据集">准备数据集：</h3>
<p>收集和准备与目标任务相关的训练数据集。确保数据集质量和标注准确性，并进行必要的数据清洗和预处理。</p>
<h3 id="选择预训练模型基础模型">选择预训练模型/基础模型：</h3>
<p>根据目标任务的性质和数据集的特点，选择适合的预训练模型。</p>
<h3 id="设定微调策略">设定微调策略：</h3>
<p>根据任务需求和可用资源，选择适当的微调策略。考虑是进行全微调还是部分微调，以及微调的层级和范围。</p>
<h3 id="设置超参数">设置超参数：</h3>
<p>确定微调过程中的超参数，如学习率、批量大小、训练轮数等。这些超参数的选择对微调的性能和收敛速度有重要影响。</p>
<h3 id="初始化模型参数">初始化模型参数：</h3>
<p>根据预训练模型的权重，初始化微调模型的参数。对于全微调，所有模型参数都会被随机初始化；对于部分微调，只有顶层或少数层的参数会被随机初始化。</p>
<h3 id="进行微调训练">进行微调训练：</h3>
<p>使用准备好的数据集和微调策略，对模型进行训练。在训练过程中，根据设定的超参数和优化算法，逐渐调整模型参数以最小化损失函数。</p>
<h3 id="模型评估和调优">模型评估和调优：</h3>
<p>在训练过程中，使用验证集对模型进行定期评估，并根据评估结果调整超参数或微调策略。这有助于提高模型的性能和泛化能力。</p>
<h3 id="测试模型性能">测试模型性能：</h3>
<p>在微调完成后，使用测试集对最终的微调模型进行评估，以获得最终的性能指标。这有助于评估模型在实际应用中的表现。</p>
<h3 id="模型部署和应用">模型部署和应用：</h3>
<p>将微调完成的模型部署到实际应用中，并进行进一步的优化和调整，以满足实际需求。</p>

    </div>
    <div class="post-footer">
        <div class="info">
            
            <span class="separator"><a class="tag" href="/tags/ai/">AI</a><a class="tag" href="/tags/llm/">LLM</a></span>
        </div>
        


    </div>
    
        
    
</div>

                <div class="grow"></div>
                <div class="built-with">
    Built with <a href="https://gohugo.io/">Hugo</a> <b>·</b> Using the <a href="https://github.com/LucasVadilho/heyo-hugo-theme">heyo</a> theme
</div>
            </div>
        </div>
        
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.7.1/css/all.min.css" integrity="sha512-3M00D/rn8n+2ZVXBO9Hib0GKNpkm8MSUU/e2VNthDyBYxKWG+BftNYYcuEjXlyrSO637tidzMBXfE7sQm0INUg==" crossorigin="anonymous" referrerpolicy="no-referrer" />

<script type="text/javascript">
            
            
            window.MathJax = {
                tex: {
                    inlineMath: [['$', '$'], ['\\(', '\\)']],
                    displayMath: [['$$','$$'], ['\\[', '\\]']]
                },
                svg: {
                    scale: 1.25,
                }
            };
        </script><script defer type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.1.0/es5/tex-mml-svg.min.js" integrity="sha512-/mL9Gs6E5Bz6NtPOr9eY&#43;T8IIdJbo2JL3TudApzFFelwBXEc3TeFLU6kPq122TJROv7jkktuBRkz5h8vGzrsyA==" crossorigin="anonymous"></script>
    </body>
</html>